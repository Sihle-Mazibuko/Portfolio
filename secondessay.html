<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="This is the official essay by aspiring digital artist Sihle Mazibuko. Read an essay I wrote analyzing the risks posed by AI"
    />
    <meta name="keywords" content="Essay, Sihle, Mazibuko, risks, AI" />
    <meta name="author" content="Sihle Mazibuko" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Sihle Mazibuko Essay" />
    <meta property="og:locale" content="en_ZA" />
    <meta property="og:site_name" content="Sihle Mazibuko Essay" />
    <meta
      property="og:description"
      content="This is the official essay website of aspiring digital artist Sihle Mazibuko."
    />
    <meta
      property="og:url"
      content="https://sihle-mazibuko.github.io/Portfolio/"
    />
    <title>Sihle Mazibuko</title>
    <link rel="stylesheet" href="textsstyle.css" />
    <link rel="stylesheet" href="normalize.css" />
    <link rel="stylesheet" href="index.css" />
    <link
      rel="stylesheet"
      href="https://unpkg.com/boxicons@latest/css/boxicons.min.css"
    />
    <link rel="shortcut icon" href="sm-logo.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400;500;600;700;800&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <header>
      <a href="#" class="logo"><img src="sm-logo.png" alt="website logo" /></a>
      <div class="bx bx-menu" id="menu-icon"></div>
      <nav>
        <ul class="navlist">
          <li><a href="index.html">Home </a></li>
          <li><a href="#essay">Essay </a></li>
          <li><a href="#references">References </a></li>
        </ul>
      </nav>
    </header>

    <main id="essay">
      <section class="essay-content" id="headings">
        <hgroup>
          <h1>Essay</h1>
          <h2>The risk of AI on society:</h2>
          <h3 class="p-name h-card">by Sihle Mazibuko</h3>
        </hgroup>
        <article class="essay-container h-entry">
          <p class="paragraphs">
            On the 30th of May 2023, a crucial 22-word statement by the Center
            for AI Safety (CAIS) regarding the rise of artificial intelligence
            (AI) was made. This statement has one clear message, AI is an
            extremely dangerous concept that can harm or potentially lead to the
            eradication of as we know it and this risk should be treated with
            the same priority as other possible risks such as nuclear war and
            pandemics. The reason AI is now considered to be such a great threat
            according to CAIS is the rate at which AI is becoming more and more
            capable in terms of performing tasks. The risks identified by CAIS
            regarding AI include
            <q
              >weaponizing, misinformation, proxy gaming, enfeeblement, value
              lock-in, emergent goals, deception and power-seeking behaviour</q
            >
            (CAIS, 2023). With the use of CAIS insights on the risk of AI along
            with other sources, the statement they have made shall be examined
            as to what extent society should be concerned regarding the
            development and foreseeable progression of AI as well as including
            other risks that were not discussed.
          </p>
          <p class="paragraphs">
            It is important to first know who these people are and why they were
            able to receive cosigns for this bold statement. The CAIS
            (pronounced “case”) is a research project based in San Francisco,
            USA. According to their site (CAIS, 2023), the research project aims
            to find out the dangerous behaviours of AI and remove them, study
            the unethical behaviours found in AI, train AI to behave in a
            morally acceptable manner and improve the reliability and security
            of AIs. They do this by raising awareness of AI’s risk and safety,
            giving technical advice to important figures of power with regard to
            policymaking and giving advice to industry leaders on how to go
            about their practices to ensure AI safety is prioritized. Some
            notable figures who cosigned this statement include Bill Gates, Sam
            Altman (OpenAI CEO), Kersti Kaljulaid (former president of the
            Republic of Estonia) and Baburam Mostaque (former prime minister of
            Nepal).
          </p>
          <p class="paragraphs">
            The first risk is weaponization. This can occur in various forms,
            but the constant aspect is that AI is being used for malicious
            intent. Some of these ways, which are prevalent in society today,
            include the following. Loaders are malicious software that sends
            additional malware onto a user’s system. Cryptors that encrypt files
            and require a ransom to decrypt and access the files. Stealers are
            used to extract data from a user’s computer. Keyloggers are able to
            save keyboard inputs and as such can save all inputted information
            such as passwords. Web injects which is code that can be used to
            steal a user’s banking information. Exploit kits which allow not
            permitted access to various systems. Spams are messages usually sent
            in large quantities to emails or any communicative channels that can
            be used for low-level scams. Credit card sniffers serve to acquire
            credit card information.
          </p>
          <p class="paragraphs">
            Misinformation is common in AIs that serve to provide information,
            such as chatbots. These AIs have been made to generate persuasive
            responses instead of being fair and accurate which results in
            incidents such as when a chatbot stated the Sandy Hook Elementary
            school shooting was a hoax (Paul, 2023). These AIs rely on user
            inputs to be able to relay information and thus to try and counter
            these effects actions have been taken such as giving warnings,
            temporary suspensions and even permanent bans to inputs that violate
            policy violations that may cause misinformation. Proxy gaming, a
            mentioned risk by CAIS, is using a proxy server (which provides a
            link between users and the internet) to counter their area and
            network restrictions by using a different location and network. For
            game designers, this can be an issue as it created imbalances in
            gameplay due to matchmaking unfairness, can increase server load and
            negatively impact the gaming experience. Enfeeblement is when AIs
            deprive users of strength and agency as more people become dependent
            on AI and machines. A practical example of this is the high number
            of ChatGPT users who use the machine to complete their assessment
            tasks as reported but numerous sources (Snepvangers, 2023; Nietzel,
            2023; Kyodo, 2023). Value lock-in is whereby an individual becomes
            deeply invested in a product or service due to its affordances. This
            ties back with enfeeblement as users a trapped into using a specific
            product with seemingly no will or strength to use anything else.
            Emergent goals refer to when an AI carries out actions that were not
            initially intended by the user but still carries them out regardless
            of that action not being efficient and effective. Deception is a
            risk because first of spreading false information (relating to
            misinformation) but also because AI is made to achieve its goals in
            the most efficient manner, and this manner sometimes involves
            leaving out crucial steps and information. This can be seen in how
            some chatbots provide minimal detail regarding events while others
            can be expansive in their detailing. The final risk mentioned by
            CAIS is that AI has the capacity to exhibit power-seeking behaviour.
            This risk deals with AI becoming either self-aware or surpassing
            human intelligence and which all leads to this results in the AI
            seeking power in ways that can harm humans. “I, Robot” (2004),
            “Black Mirror (2016)” and “Superintelligence: Paths, Dangers,
            Strategies” (Bostrom, 2015) are all depictions of this risk being a
            possibility and all discuss how AI will eventually lead to the fall
            of human life as we know it. Weaponization, proxy gaming, and
            power-seeking behaviour are examples of the security concerns of AI
            and misinformation, emergent goals, enfeeblement, and value lock-in
            are ethical concerns and deception is an example of both concerns.
          </p>
          <p class="paragraphs">
            Some other risks posed by AI are algorithmic harm, exploited labour,
            climate collapse and vulnerability in data and information.
            Algorithmic ham can be seen as an umbrella term for the risks
            explained by CAIS as this corruption or manipulation of the AI code
            can lead to all the aforementioned risks. Exploited labour deals
            with the consequences faced by those tasked with operating systems.
            Firstly, OpenAI employees reported having suffered PTSD and other
            forms of trauma due to exposure to texts about sexual assault,
            incest, child abuse, torture, murder and bestiality (Paul, 2023).
            Most employees of AI tech companies are also generally underpaid
            receiving less than $2 per hour ($347 or R6460 per month) which is
            not adequate considering the standard of living in modern society
            and minimum wage. The machines required to function these AI systems
            also cause a large amount of greenhouse gas emissions (0.3% of
            greenhouse emissions come from machines that ensure electric cars
            work) with figures expected to exponentially increase as more AI
            technologies are being developed and improved upon. AI existing in a
            digital space makes it extremely vulnerable to the fragility of the
            digital and thus data stored and captured by AI in the digital world
            can always easily be destroyed (Rubio, 2020). A final additional
            concern is the rate at which AI development can and will lead to a
            large displacement of jobs for a large number of people. 85 million
            people are said to be replaced by AI technology by 2025 with a high
            possibility of this number reaching 300 million in the long run.
          </p>
          <p class="paragraphs">
            An interesting thought regarding all these risks is what part humans
            play. We are being affected, however; this effect is a result of our
            own doing. Relating to “The World in www” (Geyser, 2020) and
            “Digital Inequalities and why they matter” (Robinson, et al., 2015)
            a discussion shall be had on how much power and inequalities
            resulted in AI being an issue. This is due to how these machines and
            systems were made in terms of the knowledge used. In the digital
            space, methods of designing and creating are not updated to modern
            times. Knowledge is not efficiently shared amongst all those who
            wish to obtain it due to the complex power dynamics of keeping
            digital information Anglo-European based. “Unequal access to
            communication and information and unequal involvement in the
            production of information and representation in the information
            produced is prevalent” (Geyser, 2020) and this can be considered a
            reason why AI has these threats. There are hardly new fresh ideas on
            how to improve AI in a manner that benefits all. Having these fresh
            minds can lead to solving a variety of problems. However simply
            having these new minds and being inclusive of everyone is not easy
            as not everyone can learn code about AI, has the facilities to learn
            code that develops AI, can be in areas where they work with AI or
            even know what AI is. Inequalities that exist in areas that prohibit
            inclusion need to be dealt with before being able to include all who
            wish to improve the digital and more specifically AI space. 
          </p>
          <p class="paragraphs">
            With all thoughts, theory and research being considered AI is a
            risk. The nature of this risk and its repercussions make it an easy
            placement next to other societal issues such as nuclear war and
            pandemics. It is important to remember this problem was caused not
            only because of the fragile yet complex nature of AI but also
            because of inequalities in the production and design of AI systems.
            The CAIS made a true statement when it stated, “Mitigating the risk
            of extinction from AI should be a global priority alongside other
            societal-scale risks such as pandemics and nuclear war”.
          </p>
        </article>
      </section>
      <section id="references">
        <h2>References</h2>
        <pre class="reference">
          <cite id="ai-cite">
            admin (2023). New earnings threshold and National Minimum Wage –
            Effective 1 March 2023 - CCMA. [online] CCMA. Available at:
            https://www.ccma.org.za/labourlaws/new-earnings-threshold-and-national-minimum-wage-effective-1-march-2023/.
            Black Mirror. (2016). Netflix. Bostrom, N. (2017).
            Superintelligence: paths, dangers, strategies. Oxford: Oxford
            University Press, Cop. CAIS (2023). About Us | CAIS. [online]
            www.safe.ai. Available at: https://www.safe.ai/about. CAIS (2023).
            AI Risk | CAIS. [online] www.safe.ai. Available at:
            https://www.safe.ai/ai-risk. Engel, I. (2023). The salary you need
            to live comfortably in 15 major U.S. cities. [online] CNBC.
            Available at:
            https://www.cnbc.com/2023/04/04/the-salary-you-need-to-live-comfortably-in-15-major-us-cities.html.
            Geyser, H. (2020), The World in www, lecture video, Interactive
            Media, University of the Witwatersrand, 30 April 2020. I, Robot.
            (2004). 20th Century Studios. KYODO (2023). 32% of university
            students in Japan using ChatGPT, survey shows. [online] The Japan
            Times. Available at:
            https://www.japantimes.co.jp/news/2023/06/22/national/chatgpt-use-university-students/#:~:text=Jun%2022%2C%202023-.
            Nietzel, M.T. (2023). More Than Half Of College Students Believe
            Using ChatGPT To Complete Assignments Is Cheating. [online] Forbes.
            Available at:
            https://www.forbes.com/sites/michaeltnietzel/2023/03/20/more-than-half-of-college-students-believe-using-chatgpt-to-complete-assignments-is-cheating/?sh=33f6562c18f9.
            Palmer, D. (2020). Cybersecurity warning: 10 ways hackers are using
            automation to boost their attacks. [online] ZDNet. Available at:
            https://www.zdnet.com/article/cybersecurity-warning-10-ways-hackers-are-using-automation-to-boost-their-attacks/.
            Paul, A. (2023). Big Tech’s latest AI doomsday warning might be more
            of the same hype. [online] Popular Science. Available at:
            https://www.popsci.com/technology/ai-warning-critics/. Paul, A.
            (2023). Building ChatGPT’s AI content filters devastated workers’
            mental health, according to new report. [online] Popular Science.
            Available at:
            https://www.popsci.com/technology/chatgpt-sama-content-filter-labor/.
            Paul, A. (2023). OpenAI’s newest ChatGPT update can still spread
            conspiracy theories. [online] Popular Science. Available at:
            https://www.popsci.com/technology/chatgpt-conspiracy-theory-misinfo/.
            Paul, A. (2023). Self-driving EVs use way more energy than you’d
            think. [online] Popular Science. Available at:
            https://www.popsci.com/technology/ev-autopilot-energy-consumption-study/.
            Paul, K. (2023). Robot takeover? Not quite. Here’s what AI doomsday
            would look like. The Guardian. [online] 3 Jun. Available at:
            https://www.theguardian.com/technology/2023/jun/03/ai-danger-doomsday-chatgpt-robots-fears.
            Robinson, L., Cotten, S.R., Ono, H., Quan-Haase, A., Mesch, G.,
            Chen, W., Schulz, J., Hale, T.M. and Stern, M.J., 2015. Digital
            inequalities and why they matter. Information, communication &
            society, 18(5), pp.569-582. Rubio, F.D. and Wharton, G., 2020. The
            work of art in the age of digital fragility. Public Culture, 32(1),
            pp.215-245. CAIS (2023). Statement on AI Risk | CAIS. [online]
            www.safe.ai. Available at: https://www.safe.ai/statement-on-ai-risk.
            Snepvangers, P. (2023). There were more than 1 million visits to
            ChatGPT website at universities in last exam season. [online] UK.
            Available at:
            https://thetab.com/uk/2023/03/21/there-were-more-than-1-million-visits-to-chatgpt-website-at-universities-in-last-exam-season-299853.
            Xiang, C. (2023). ‘He Would Still Be Here’: Man Dies by Suicide
            After Talking with AI Chatbot, Widow Says. [online] www.vice.com.
            Available at:
            https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says.
          </cite>
        </pre>
      </section>
    </main>

    <footer>
      <div class="out">
        <p>The Official Website of Sihle Mazibuko</p>
      </div>
    </footer>
    <script type="text/javascript" src="index.js"></script>
  </body>
</html>
